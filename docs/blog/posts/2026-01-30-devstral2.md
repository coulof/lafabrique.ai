---
date:
  created: 2024-06-30
draft: true
---

# Devstral 2 & `vibe` by Mistral AI :  the hidden gems of the AI Coding Agent.

Since about a year I'm working almost daily with different coding assistants depending on my mood or constraints.

I have been able to use and test, Windsurf & Tabnine at the job ; while I was a fervant user of Copilot I switched to Claude Code.

At the begining I came across Devstral 2 & to me it replaced Claude Code in my workflow for the following reason:
1. It is beautiful ! Checkout the [blog post announcement](https://mistral.ai/fr/news/devstral-2-vibe-cli), the [API docs](https://docs.mistral.ai/) and `vibe` itself. The colorscheme, the effects, and overall quality put into it give a whole different experience. For me it's much more pleasant to work in a beautiful environment.
2. On the benchmark of me, myself & I, Devstral 2 performs as good as Claude Code. Not everything is perfect both trend to not read the docs of the framework but give real good results overall.
3. It is free (or at least way cheaper than Claude Code) & Open Source! You have 1 Million toekn for trial then ; $0.10/$0.30 for Devstral Small 2. In Claude code I'm blocked at least once a day, even after playing with `/compact` & juggling with `/usage`
4. It is slower than Claude Code but... You can run it entirely on your local machine.

The documentation to run it locally is rather loose and Devstral-2-small is still relatively greedy. Below are instructions to get it running on Ubuntu 24.04I had to die & retry a little bit before being able to run it.

Following are the instructions to on how to run Devstral 2 small + `vibe` on Ubuntu 24.04 with an NVIDIA GPU.

<!-- more -->

Devstral 2 is one the hidden gem of the AI Agent ecosystem.

Devstral 2 is Mistral AI coding model. Released in december it comes with a bunch of useful features:
1. There are two size models (Devstral 2 with 123B parameters and Devstral Small 2 with 24B)
2. As usual with Mistral AI the models are under Open Source license
3. `vibe` [CLI coding agent](https://github.com/mistralai/mistral-vibe/) :material-github:
Checkout the [official blog post](https://mistral.ai/fr/news/devstral-2-vibe-cli) for more details
The version https://mistral.ai/fr/news/devstral-2-vibe-cli


## Sidequest get it running on KVM
`C:\Users\[username]\.wslconfig`
```ini
[wsl2]
# Memory allocation - leave 28GB for Windows
memory=100GB

# Use most of my CPU cores
processors=14

# Swap space for safety during compilation
swap=24GB

# GPU passthrough (critical for vLLM)
nestedVirtualization=true

# Disable page reporting to improve memory performance
pageReporting=false

# Increase virtual disk limit if needed
[experimental]
autoMemoryReclaim=gradual
sparseVhd=true
```



I used the Build: Nightly on Platform: CUDA in version 12.9 with Package manager: `uv` and 

Other combos are easily selectable from : 
https://vllm.ai/#quick-start


Ubuntu 24.04 on WSL2
https://huggingface.co/mistralai/Devstral-Small-2-24B-Instruct-2512#benchmark-results


https://developer.nvidia.com/cuda-12-9-1-download-archive
package_update: true
package_upgrade: true

packages:
  - curl
  - git
  - python3-venv

runcmd:
  - curl -LsSf https://astral.sh/uv/install.sh | sh
  - echo "uv installed successfully" >> /var/log/cloud-init-output.log


curl -LsSf https://astral.sh/uv/install.sh | sh
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64/cuda-ubuntu2404.pin
mv cuda-ubuntu2404.pin /etc/apt/preferences.d/cuda-repository-pin-600
wget https://developer.download.nvidia.com/compute/cuda/12.9.1/local_installers/cuda-repo-ubuntu2404-12-9-local_12.9.1-575.57.08-1_amd64.deb
dpkg -i cuda-repo-ubuntu2404-12-9-local_12.9.1-575.57.08-1_amd64.deb
cp /var/cuda-repo-ubuntu2404-12-9-local/cuda-*-keyring.gpg /usr/share/keyrings/
apt-get update
apt-get -y install cuda-toolkit-12-9
nvcc --version


apt-get -y python3.12-venv
mkdir vllm-cuda-12
cd vllm-cuda-12
python3 -m venv .
source bin/activate



nvcc --version

mkdir vllm-cuda-12
cd vllm-cuda-12
python3 -m venv .
source bin/activate
uv pip install -U vllm --pre --extra-index-url https://wheels.vllm.ai/nightly/cu129 --extra-index-url https://download.pytorch.org/whl/cu129 --index-strategy unsafe-best-match


vllm serve mistralai/Devstral-Small-2-24B-Instruct-2512     --max-model-len 26214 --tensor-parallel-size 1     --tool-call-parser mistral --enable-auto-tool-choice

```ini
[[providers]]
name = "llamacpp"
api_base = "http://127.0.0.1:8000/v1"
api_key_env_var = ""
api_style = "openai"
backend = "generic"
reasoning_field_name = "reasoning_content"

[[models]]
name = "devstral-2-small"
provider = "llamacpp"
alias = "local"
temperature = 0.2
input_price = 0.0
output_price = 0.0
```

Retester avec CUDA 13

uv pip install -U vllm --pre --extra-index-url https://wheels.vllm.ai/nightly/cu130 --extra-index-url https://download.pytorch.org/whl/cu130 --index-strategy unsafe-best-match


vllm serve mistralai/Devstral-Small-2-24B-Instruct-2512 \
    --max-model-len 8192 \
    --gpu-memory-utilization 0.95 \
    --tensor-parallel-size 2 \
    --tool-call-parser mistral \
    --enable-auto-tool-choice 2>&1 | tee vllm.log


```
nvidia-smi
Tue Jan 27 14:53:40 2026
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA L40S                    Off |   00000000:01:00.0 Off |                    0 |
| N/A   40C    P0             84W /  350W |   42113MiB /  46068MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A            6409      C   VLLM::EngineCore                      42104MiB |
+-----------------------------------------------------------------------------------------+
```


<video controls preload='metadata' onclick='(function(el){ if(el.paused) el.play(); else el.pause() })(this)'>
  <source src='assets/images/vibe-install-and-colors.mp4' type='video/mp4; codecs="avc1.42E01E, mp4a.40.2"'>
</video>

---
Devstral 2 & `vibe` by Mistral AI :  the hidden gems of the AI Coding Agent.


Since about a year I'm working almost daily with different coding assistants depending on my mood or constraints.

I have been able to use and test, Windsurf & Tabnine at the job ; while I was a fervant user of Copilot I switched to Claude Code.

At the begining I came across Devstral 2 & to me it replaced Claude Code in my workflow for the following reason:
1. It is beautiful ! Checkout the blog post announcement, the API docs and `vibe` itself. The colorscheme, the effects, and overall quality put into it give a whole different experience. For me it's much more pleasant to work in a beautiful environment.
2. On the benchmark of me, myself & I, Devstral 2 performs as good as Claude Code. Not everything is perfect both trend to not read the docs of the framework but give real good results overall.
3. It is free (or at least way cheaper than Claude Code) & Open Source! You have 1 Million toekn for trial then ; $0.10/$0.30 for Devstral Small 2. In Claude code I'm blocked at least once a day, even after playing with `/compact`
4. It is slower than Claude Code but... You can run it entirely on your local machine.

The documentation to run it locally is rather loose and I had to die & retry a little bit before being able to run it.

Check out this lafabrique.ai blog for the details on how to run Devstral 2 small + `vibe` on Ubuntu 24.04 on WSL2 with an NVIDIA GPU :
