---
date: 2026-01-30
authors:
  - coulof
categories:
  - AI
  - Development
  - Tools
title: "Devstral 2 and vibe: Hidden Gems of AI Coding Agents"
draft: true
---

# Devstral 2 & `vibe` by Mistral AI : the hidden gems of the AI Coding Agent.

For about a year, I have been working daily with various coding assistants, choosing different tools depending on my needs and constraints. My journey has included testing Windsurf and Tabnine professionally, while personally transitioning from being a fervent Copilot user to adopting Claude Code.

During this exploration, I discovered Devstral 2, which ultimately replaced Claude Code in my workflow for several compelling reasons:
1. **Aesthetic Excellence**: The tool offers a beautiful user experience. From the [blog post announcement](https://mistral.ai/fr/news/devstral-2-vibe-cli) to the [API documentation](https://docs.mistral.ai/) and `vibe` itself, the color scheme, visual effects, and overall polish create a distinctly pleasant working environment.

2. **Comparable Performance**: In my personal benchmarking, Devstral 2 performs on par with Claude Code. While neither tool is perfectâ€”both occasionally overlook framework documentationâ€”they deliver excellent results overall.

3. **Cost-Effective and Open Source**: Devstral 2 is significantly more affordable than Claude Code and is open source. Users receive 1 million tokens for trial, with pricing at $0.10/$0.30 for Devstral Small 2. With Claude Code, I frequently hit usage limits, even after employing `/compact` commands and managing `/usage` settings.

4. **Local Execution Capability**: Although Devstral 2 may be slower than Claude Code, it offers a crucial advantage: the ability to run entirely on local machines, providing greater control and privacy.

The documentation to run it locally is rather loose and Devstral-2-small is still relatively greedy ; therefore needing some tweaks.

Following are the instructions to on how to run Devstral 2 small + `vibe` on Ubuntu 24.04 with an NVIDIA L40S with 24GB VRAM hosted by [Scaleway](https://www.scaleway.com/en/gpu-instances/) :simple-scaleway:.


<!-- more -->

## Installation
The main places for offline installation documentation are the [official `vibe` doc](https://docs.mistral.ai/mistral-vibe/local) :simple-mistralai: & the [huggingface Devstral-Small-2 page](https://huggingface.co/mistralai/Devstral-Small-2-24B-Instruct-2512) ðŸ¤—


First vllm is recommended way to run Devstral-Small-2. If you prefer ollama the model is ready to go as well : https://ollama.com/library/devstral-small-2

For this lab I used the Nightly build on CUDA in version 12.9 deployed `uv` Package manager.

Other combos are easily selectable from :  https://vllm.ai/#quick-start

So where do we start ?

1. With CUDA ! The instructions are straight forward and you can select your platform of choice from : https://developer.nvidia.com/cuda-12-9-1-download-archive
  In this case:
```bash
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64/cuda-ubuntu2404.pin
mv cuda-ubuntu2404.pin /etc/apt/preferences.d/cuda-repository-pin-600
wget https://developer.download.nvidia.com/compute/cuda/12.9.1/local_installers/cuda-repo-ubuntu2404-12-9-local_12.9.1-575.57.08-1_amd64.deb
dpkg -i cuda-repo-ubuntu2404-12-9-local_12.9.1-575.57.08-1_amd64.deb
cp /var/cuda-repo-ubuntu2404-12-9-local/cuda-*-keyring.gpg /usr/share/keyrings/
apt-get update
apt-get -y install cuda-toolkit-12-9
```

2. Python dependencies if you are on a the fresh Ubuntu install:
```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
apt-get -y python3.12-venv
```

3. Prepare a directory for vllm & install it
```bash
mkdir vllm-cuda-12
cd vllm-cuda-12
python3 -m venv .
source bin/activate
uv pip install -U vllm --pre --extra-index-url https://wheels.vllm.ai/nightly/cu129 --extra-index-url https://download.pytorch.org/whl/cu129 --index-strategy unsafe-best-match
```

4. Run the model
```bash
vllm serve mistralai/Devstral-Small-2-24B-Instruct-2512 \
    --max-model-len 8192 \            # 200k tokens was just too big for 48GB VRAM
    --gpu-memory-utilization 0.95 \   # Save some memory for other apps
    --tensor-parallel-size 1 \        # I only have on GPU as opposite to the default given command
    --tool-call-parser mistral \
    --enable-auto-tool-choice 2>&1 | tee vllm.log # because I trial & error
```

5. Install `vibe` 
Last bit is to patch `~/.vibe/config.toml` to use the right model with the right provider
```ini
[[providers]]
name = "llamacpp"
api_base = "http://127.0.0.1:8000/v1"
api_key_env_var = ""
api_style = "openai"
backend = "generic"
reasoning_field_name = "reasoning_content"

[[models]]
name = "devstral-2-small"
provider = "llamacpp"
alias = "local"
temperature = 0.2
input_price = 0.0
output_price = 0.0
```

From `vibe` you can select the `local` context with `/config` command.

You are now ready to code with Devstral-Small-2 !

Below is a quick take on the GPU usage while serving blazing fast tokens :
```
nvidia-smi
Tue Jan 27 14:53:40 2026
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA L40S                    Off |   00000000:01:00.0 Off |                    0 |
| N/A   40C    P0             84W /  350W |   42113MiB /  46068MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A            6409      C   VLLM::EngineCore                      42104MiB |
+-----------------------------------------------------------------------------------------+
```

<video controls preload='metadata' onclick='(function(el){ if(el.paused) el.play(); else el.pause() })(this)'>
  <source src='assets/images/vibe-install-and-colors.mp4' type='video/mp4; codecs="avc1.42E01E, mp4a.40.2"'>
</video>


## Sidequest running Devstral-small-2 on KVM & a 5070Ti
Spoiler it failed but I got a couple of takeways, first how to tweak the wsl2 settings with `C:\Users\[username]\.wslconfig` (complete list [here](https://learn.microsoft.com/en-us/windows/wsl/wsl-config):material-microsoft-windows-classic:).


```ini
[wsl2]
# Memory allocation - leave 28GB for Windows
memory=100GB

# Use most of my CPU cores
processors=14

# Swap space for safety during compilation
swap=24GB

# GPU passthrough (critical for vLLM)
nestedVirtualization=true

# Disable page reporting to improve memory performance
pageReporting=false

# Increase virtual disk limit if needed
[experimental]
autoMemoryReclaim=gradual
sparseVhd=true
```

Or how to offload some of the GPU inference workload to the CPU:
```bash
vllm serve mistralai/Devstral-Small-2-24B-Instruct-2512 \
    --max-model-len 2048 \
    --gpu-memory-utilization 0.95 \
    --cpu-offload-gb 10 \
    --tool-call-parser mistral \
    --enable-auto-tool-choice
```

> Disclaimer, this blog post has been proof-read with... Devstral using the guidelines from lafabrique.ai :material-factory: [github repo](https://github.com/coulof/lafabrique.ai/tree/main/.ai) :simple-github: